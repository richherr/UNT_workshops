---
title: "An Introduction to Conda and Mamba on LS6"
subtitle: "Streamlining Scientific Computing with Effective Package Management"
author: "Rich Herrington"
format: 
  revealjs: 
    theme: white
    transition: slide
    self-contained: true
code-tools: true
incremental: true
title-slide-attributes: 
  data-background-image: unt.png
  data-background-size: 15%
  data-background-position: 2% 2%
editor: 
  markdown: 
    wrap: 72
---

# Slide URL

[https://richherr.github.io/UNT_workshops/](https://richherr.github.io/UNT_workshops/){style="font-size: 1.25em"}

## Conda for High-Performance Computing (HPC)

-   Conda: An open-source package management and environment management
    system.
-   Purpose: Facilitates the installation, management, and deployment of
    software packages and libraries.
-   Cross-Platform: Available for Windows, macOS, and Linux.

## Conda in the Context of HPC

-   Environment Management: Conda allows users to create isolated
    environments to manage dependencies and versions of software,
    ensuring consistency and reproducibility across different stages of
    the project and among different team members.
-   Package Management: It simplifies the process of installing,
    updating, and managing software packages from various repositories,
    which can be crucial for managing complex HPC applications.

# Key Features Beneficial for HPC

## Reproducibility

-   Ensures that computational experiments and applications are
    reproducible by managing specific versions of packages and
    dependencies.
-   Facilitates sharing of environments using environment.yml files,
    ensuring that other researchers can recreate the same computational
    environment.

## Dependency Management

-   Automatically resolves and manages dependencies, ensuring that all
    software libraries are compatible.
-   Reduces the dependency problems by handling library versions and
    resolving conflicts, which is crucial in HPC where multiple
    libraries might be used for parallel computing, numerical analysis,
    data processing, etc.

## Cross-Language Support

-   Supports various programming languages like Python, R, Julia, Scala,
    C/C++, SQL, etc., which are commonly used in data analysis, machine
    learning, scientific computing, and more in the HPC context.
-   Allows for the integration of packages from different languages into
    a single computational environment, facilitating cross-language
    development and execution.

## Binary Package Management:

-   Provides pre-compiled binary packages, which reduces the need to
    compile code and manage compilers, thereby simplifying software
    deployment on HPC systems.
-   Provides binaries that are compatible with various CPU and GPU
    architectures, ensuring wide applicability in diverse HPC setups.
-   Conda supports binary packages for Windows, macOS, and Linux,
    ensuring that software can be consistently deployed across various
    platforms.

## Channel and Repository Management:

-   Allows users to access packages from different repositories and
    channels, providing a wide array of scientific and computational
    libraries and tools optimized for various use cases.
-   Enables organizations to host their own repositories to manage and
    distribute software efficiently.

## Local Software Management

-   Conda allows users to create isolated environments, enabling the
    installation of software packages and dependencies without
    interfering with system-wide installations or requiring
    administrator privileges.
-   Users can install software packages locally within a Conda
    environment, bypassing the need for system administrator permissions
    usually required for global installations.
-   Conda provides access to pre-compiled binary packages, eliminating
    the need to compile software from source which might require
    system-level dependencies and administrator privileges.

## One Last Important Point About Performance

::: fragment
::: callout-warning
Conda and its generic binary packages may not always provide the level
of optimization and performance tuning that specialized HPC compilers
and networking solutions can offer. The convenience and ease of use
provided by Conda might come at the cost of potentially lower
performance in some HPC scenarios. However, Conda still remains a
valuable tool for managing and deploying software, especially in
environments where ease of use, reproducibility, and rapid deployment
are prioritized over squeezing out every last bit of performance.
:::
:::

# Installing Conda on LS6

## Download and Installation {.smaller style="font-size: 0.63em"}

1.  Change into your work directory; then download the miniforge
    mamba/conda installer from the github site.
    <https://github.com/conda-forge/miniforge>

::: fragment
``` bash
login1.ls6(672)$ pwd
/home1/07723/richherr
login2.ls6(549)$ cd /home1/07723/richherr/work
login1.ls6(658)$ wget "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
```
:::

2.  Change the permissions on the installer to executable (+x).\
    Then run the executable and follow the directions. Make sure to use
    the path to your **work** directory when installing. You don't want
    to install into your **home** directory.

3.  Respond **yes** the question: "Do you wish the installer to
    initialize Miniforge3 by running conda init? \[yes\|no\]"

::: fragment
``` bash
login1.ls6(670)$ pwd
/home1/07723/richherr/work
login1.ls6(662)$ ls -la Miniforge3-Linux-x86_64.sh
-rw------- 1 richherr G-824114 87007851 Sep  3 18:22 Miniforge3-Linux-x86_64.sh
login1.ls6(663)$ chmod +x Miniforge3-Linux-x86_64.sh
login1.ls6(664)$ ls -la Miniforge3-Linux-x86_64.sh
-rwx------ 1 richherr G-824114 87007851 Sep  3 18:22 Miniforge3-Linux-x86_64.sh
login1.ls6(665)$ ./Miniforge3-Linux-x86_64.sh

Welcome to Miniforge3 23.3.1-1

In order to continue the installation process, please review the license
agreement.
Plea se, press ENTER to continue
>>>
```
:::

## Creating a [conda.init]{style="color: red;"} File {.smaller style="font-size: 0.68em"}

4.  We need to remove the conda lines added to your **.bashrc** file and
    place them in a seperate file called **conda.init**. We'll grab the
    last 20 lines to make sure we get all of the lines. We may need to
    delete some of the lines that are not part of the conda init lines.
    Use **nano** to edit the file. Scroll through the file and **Ctl-k**
    (delete) any lines that are before the conda init lines then
    **Ctl-o** to save the file. Your **conda.init** file should contain
    something similar to the lines displayed using the **more** command.

::: fragment
``` bash
login2.ls6(564)$ cd ~
login2.ls6(564)$ tail -20 .bashrc > conda.init
login2.ls6(564)$ nano conda.init [delete any lines not contained within the conda init lines]
login2.ls6(564)$ more conda.init

## >>> conda initialize >>>
## !! Contents within this block are managed by 'conda init' !!
   __conda_setup="$('/work/07723/richherr/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null
)"
   if [ $? -eq 0 ]; then
       eval "$__conda_setup"
   else
       if [ -f "/work/07723/richherr/miniconda3/etc/profile.d/conda.sh" ]; then
           . "/work/07723/richherr/miniconda3/etc/profile.d/conda.sh"
       else
           export PATH="/work/07723/richherr/miniconda3/bin:$PATH"
       fi
   fi
   unset __conda_setup
## <<< conda initialize <<<
```
:::

## Editing [.bashrc]{style="color: red;"} and Activating Conda {style="font-size: 0.80em"}

5.  Now we need to remove the conda init lines from our **.bashrc**
    file. Use **nano** editor again to edit the **.bashrc** file. Scroll
    to the bottom of the file and use **Ctl-k** to delete all of the
    conda init lines (see lines above). When finished use **Ctl-o** to
    save to your **.bashrc** file. Now you can use the **source**
    command to source the **conda.init** file rather than having conda
    active everytime you login to LS6. This gives you the ability to use
    **conda activate** in a bash shell.

::: fragment
``` bash
login2.ls6(564)$ cd ~
login2.ls6(564)$ nano .bashrc [delete all of the conda init lines - see above]
login2.ls6(564)$ source conda.init
login2.ls6(564)$ conda activate
(base) login1.ls6(622)$ conda env list
# conda environments:
#
base                  *  /work/07723/richherr/miniconda3
>>>
```
:::

## Creating a New Conda Environment {style="font-size: 0.80em"}

6.  Let's create a new conda environment. But first, let's make sure our
    python path is the one determined by our conda base environment by
    using the **which** command. Then we will use the **create**
    argument to conda to create a new environment. Will create an
    environment for the AI software H2o.  We will use Python version 3.8 
    with H2o.
    
    [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html) 

::: fragment
``` bash
(base) login1.ls6(783)$ which python
/work/07723/richherr/miniconda3/bin/python
```
:::

::: fragment
``` bash
(base) login1.ls6(786)$ conda create -n h2oai python=3.8

Collecting package metadata (current_repodata.json): done
Solving environment: done
## Package Plan ##
  environment location: /work/07723/richherr/miniconda3/envs/h2oai

(base) login1.ls6(786)$ conda activate h2oai
(base) login1.ls6(786)$ conda install -c h2oai h2o
```
:::

## Conda Installation of H2o

7.  The **conda install** will take a few minutes to install you will
    see

::: fragment
``` bash
The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    alsa-lib-1.2.10            |       hd590300_0         542 KB  conda-forge
    cairo-1.16.0               |    h0c91306_1017         1.1 MB  conda-forge
    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge
...
...
...
  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003
  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007
  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0
  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5
  zstd               conda-forge/linux-64::zstd-1.5.5-hfc55251_0


Proceed ([y]/n)? y


Downloading and Extracting Packages

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(h2oai) login1.ls6(799)$
```
:::

## Mamba: A Fast, Reliable Alternative to Conda 

8. Mamba is an open-source package manager that is fully compatible with Conda. Like Conda, it is designed to manage and deploy software packages and their dependencies. Mamba is notably recognized for its speed in resolving dependencies and installing packages.

::: fragment
::: callout-note
Mamba was installed along with Conda during our earlier Miniforge installation
:::
:::

## Why Use Mamba Rather Than Conda?  
- Speed and Efficiency: Mamba uses a C++ library (libsolv) for dependency resolution, which is significantly faster than Conda's Python-based resolver. 
- Quick Installations: Mamba often installs packages more quickly than Conda, enhancing user efficiency, especially when dealing with environments that have numerous or complex dependencies.
- Compatibility with Conda: Mamba is compatible with Conda environments and repositories, allowing users to benefit from the extensive Conda package ecosystem without major modifications.

## Why Use Mamba Rather Than Conda?  
- Environment Management: Mamba can manage Conda environments, ensuring that users can switch between Mamba and Conda easily. 
- Lower Memory Usage: Mamba typically uses less memory than Conda, making it a lightweight alternative, especially beneficial for systems with limited resources.
- Optimized Computations: Mamba is designed to optimize computational resources, ensuring efficient use of CPU and memory during package management tasks.

## Using Mamba to install Within an Existing Conda Environment {style="font-size: 0.68em"}

8.  Let's use Mamba to install the Python module **dask** in our existing h2o conda environment. Dask is a parallel computing library in Python that enables performance optimizations through dynamic task scheduling and parallel execution, particularly suited for processing larger-than-memory datasets. 
    
    [https://docs.dask.org/en/stable/](https://docs.dask.org/en/stable/) 

::: fragment
``` bash
(h2oai) login1.ls6(799)$ mamba install dask
Downloading and Extracting Packages

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
```
:::

## Using Mamba to install Within an Existing Conda Environment {style="font-size: 0.68em"}

::: fragment
We can use **conda list** to list all the packages in the **h2oai** environment. 
:::

::: fragment
``` bash
(h2oai) c307-012.ls6(541)$ conda list
# packages in environment at /work/07723/richherr/miniconda3/envs/h2oai:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       2_gnu    conda-forge
alsa-lib                  1.2.10               hd590300_0    conda-forge
aws-c-auth                0.7.4                hc8144f4_1    conda-forge
...
...
...
zict                      3.0.0              pyhd8ed1ab_0    conda-forge
zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
zlib                      1.2.13               hd590300_5    conda-forge
zstd                      1.5.5                hfc55251_0    conda-forge
```
:::

## Using Our H2o Conda Environment with a SLURM Batch Job {style="font-size: 0.64em"}

9. Next we will use our H2o conda environment with the SLURM batch system on LS6.  We will build a build a randomForest classifier with H2o using the **Data Expo 2009 - Airline on-time performance** data. [https://community.amstat.org/jointscsg-section/dataexpo/dataexpo2009](https://community.amstat.org/jointscsg-section/dataexpo/dataexpo2009). Our target variable is whether or not a flight has been delayed.  We use a smaller subset of this data - 2GB compressed.  The full data is 12GB compressed. The smaller data has 43978 rows with 31 variables - we will use 8 variables as predictors.   We use a subset of these variables. Our Python script **h2o_randomForesty.py** is listed below.

::: fragment
``` bash
import h2o
from h2o.estimators import H2ORandomForestEstimator
# Initialize H2O cluster
h2o.init(ip="localhost", port=54321)
# Load the airlines dataset
data = h2o.import_file(path="http://h2o-public-test-data.s3.amazonaws.com/smalldata/airlines/allyears2k_headers.zip")
# Define features and target
features = ["Year", "Month", "DayOfWeek", "DepTime", "UniqueCarrier", "Origin", "Dest", "Distance"]
target = "IsArrDelayed"
# Train a model
model = H2ORandomForestEstimator()
model.train(x=features, y=target, training_frame=data)
# Train a model
model = H2ORandomForestEstimator(ntrees=100, max_depth=50)
model.train(x=features, y=target, training_frame=data)
# Display model performance
print(model)
# Shutdown H2O cluster
h2o.cluster().shutdown()
```
:::

## Creating the SLURM Batch Job {style="font-size: 0.64em"}

10. We need to create a SLURM batch script **h2o_randomForest.sh** that activates our **h2oai** environment and then submits the Python script to a compute note. 

::: fragment
``` bash
#!/bin/bash
#SBATCH --job-name=h2o_Job
#SBATCH --output=h2o_Job_%A_%a.out
#SBATCH --error=h2o_Job_%A_%a.err
#SBATCH -N 1
#SBATCH -p development
#SBATCH -n 1
#SBATCH -t 02:00:00
#SBATCH --mail-user=richherr@unt.edu
#SBATCH --mail-type=begin
#SBATCH --mail-type=end

# Load module or activate conda environment
source ~/conda.init
conda activate h2oai

# Initialize H2O cluster
srun python h2o_randomForest.py
```
:::

## Submitting the SLURM Batch Job {style="font-size: 0.64em"}

11. Make sure the python script and the slurm script are in the same directory before submitting the batch script 

::: fragment
``` bash
login1.ls6(616)$ pwd
/home1/07723/richherr/work/h2o
login1.ls6(617)$ sbatch h2o_randomForest.sh

-----------------------------------------------------------------
           Welcome to the Lonestar6 Supercomputer
-----------------------------------------------------------------

No reservation for this job
--> Verifying valid submit host (login1)...OK
--> Verifying valid jobname...OK
--> Verifying valid ssh keys...OK
--> Verifying access to desired queue (development)...OK
--> Checking available allocation (TRA21001)...OK
--> Verifying that quota for filesystem /home1/07723/richherr is at  8.61% allocated...OK
--> Verifying that quota for filesystem /work/07723/richherr/ls6 is at 12.70% allocated...OK
Submitted batch job 1218654
```
:::

::: fragment
``` bash
login1.ls6(617)$ squeue -u richherr      

login1.ls6.tacc.utexas.edu: 
Mon Oct  9 
01:36:02 2023

  JOBID    PARTITION          NAME     USER  ST      TIME  NODES           NODELIST(REASON)
1218502       normal  h2o_arrayJob richherr   R   0:01:04      1           c302-017
```
:::

## Logging Into the Compute Node and Checking the Resource Utilization {style="font-size: 0.64em"}

12. We want to make sure that our H2o randomForest model is maximally using the resources on the compute node.   To do this we ssh login to the node and use **htop** to view the processor activity.  Here we see that all of the processors are being utilized. 

::: fragment
``` bash
  login1.ls6(617)$ ssh c302-017
c302-017.ls6(608)$ htop
```
:::

::: fragment
![htop view](/work/richherr/UNT_workshops/h2o_htop_view.png){.fragment width="860" height="425"}
:::
