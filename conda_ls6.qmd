---
title: "An Introduction to Conda and Mamba on TACC LoneStar6"
subtitle: "Streamlining Scientific Computing with Effective Package Management"
author: "Rich Herrington <br> DRI, Research Computing Services"
format: 
  revealjs: 
    theme: white
    transition: slide
    self-contained: true
code-tools: true
incremental: true
title-slide-attributes: 
  data-background-image: unt.png
  data-background-size: 23%
  data-background-position: 2% 2%
editor: 
  markdown: 
    wrap: 72
---

# Slide URL

[https://richherr.github.io/UNT_workshops/](https://richherr.github.io/UNT_workshops/){style="font-size: 1.25em"}

## Conda for High-Performance Computing (HPC)

-   Conda: An open-source package management and environment management
    system.
-   Purpose: Facilitates the installation, management, and deployment of
    software packages and libraries.
-   Cross-Platform: Available for Windows, macOS, and Linux.

## Conda in the Context of HPC

-   Environment Management: Conda allows users to create isolated
    environments to manage dependencies and versions of software,
    ensuring consistency and reproducibility across different stages of
    the project and among different team members.
-   Package Management: It simplifies the process of installing,
    updating, and managing software packages from various repositories,
    which can be crucial for managing complex HPC applications.

# Key Features Beneficial for HPC

## Reproducibility

-   Ensures that computational experiments and applications are
    reproducible by managing specific versions of packages and
    dependencies.
-   Facilitates sharing of environments using environment.yml files,
    ensuring that other researchers can recreate the same computational
    environment.

## Dependency Management

-   Automatically resolves and manages dependencies, ensuring that all
    software libraries are compatible.
-   Reduces the dependency problems by handling library versions and
    resolving conflicts, which is crucial in HPC where multiple
    libraries might be used for parallel computing, numerical analysis,
    data processing, etc.

## Cross-Language Support

-   Supports various programming languages like Python, R, Julia, Scala,
    C/C++, SQL, etc., which are commonly used in data analysis, machine
    learning, scientific computing, and more in the HPC context.
-   Allows for the integration of packages from different languages into
    a single computational environment, facilitating cross-language
    development and execution.

## Binary Package Management:

-   Provides pre-compiled binary packages, which reduces the need to
    compile code and manage compilers, thereby simplifying software
    deployment on HPC systems.
-   Provides binaries that are compatible with various CPU and GPU
    architectures, ensuring wide applicability in diverse HPC setups.
-   Conda supports binary packages for Windows, macOS, and Linux,
    ensuring that software can be consistently deployed across various
    platforms.

## Channel and Repository Management:

-   Allows users to access packages from different repositories and
    channels, providing a wide array of scientific and computational
    libraries and tools optimized for various use cases.
-   Enables organizations to host their own repositories to manage and
    distribute software efficiently.

## Local Software Management

-   Conda allows users to create isolated environments, enabling the
    installation of software packages and dependencies without
    interfering with system-wide installations or requiring
    administrator privileges.
-   Users can install software packages locally within a Conda
    environment, bypassing the need for system administrator permissions
    usually required for global installations.
-   Conda provides access to pre-compiled binary packages, eliminating
    the need to compile software from source which might require
    system-level dependencies and administrator privileges.

## One Last Important Point About Performance

::: fragment
::: callout-warning
Conda and its generic binary packages may not always provide the level
of optimization and performance tuning that specialized HPC compilers
and networking solutions can offer. The convenience and ease of use
provided by Conda might come at the cost of potentially lower
performance in some HPC scenarios. However, Conda still remains a
valuable tool for managing and deploying software, especially in
environments where ease of use, reproducibility, and rapid deployment
are prioritized over squeezing out every last bit of performance.
:::
:::

# Installing Conda on LS6

## Download and Installation {.smaller style="font-size: 0.63em"}

1.  Change into your work directory; then download the miniforge
    mamba/conda installer from the github site.
    <https://github.com/conda-forge/miniforge>

::: fragment
``` bash
login1.ls6$ pwd
/home1/07723/richherr
login2.ls6$ cd /home1/07723/richherr/work
login1.ls6$ wget "https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh"
```
:::

2.  Change the permissions on the installer to executable (+x).\
    Then run the executable and follow the directions. Make sure to use
    the path to your **work** directory when installing. You don't want
    to install into your **home** directory.

3.  Respond **yes** the question: "Do you wish the installer to
    initialize Miniforge3 by running conda init? \[yes\|no\]"

::: fragment
``` bash
login1.ls6$ pwd
/home1/07723/richherr/work
login1.ls6$ ls -la Miniforge3-Linux-x86_64.sh
-rw------- 1 richherr G-824114 87007851 Sep  3 18:22 Miniforge3-Linux-x86_64.sh
login1.ls6$ chmod +x Miniforge3-Linux-x86_64.sh
login1.ls6$ ls -la Miniforge3-Linux-x86_64.sh
-rwx------ 1 richherr G-824114 87007851 Sep  3 18:22 Miniforge3-Linux-x86_64.sh
login1.ls6$ ./Miniforge3-Linux-x86_64.sh

Welcome to Miniforge3 23.3.1-1

In order to continue the installation process, please review the license
agreement.
Please, press ENTER to continue
>>>
```
:::

## Creating a [conda.init]{style="color: red;"} File {.smaller style="font-size: 0.68em"}

4.  We need to remove the conda lines added to your **.bashrc** file and
    place them in a seperate file called **conda.init**. We'll grab the
    last 20 lines to make sure we get all of the lines. We may need to
    delete some of the lines that are not part of the conda init lines.
    Use **nano** to edit the file. Scroll through the file and **Ctl-k**
    (delete) any lines that are before the conda init lines then
    **Ctl-o** to save the file. Your **conda.init** file should contain
    something similar to the lines displayed using the **more** command.

::: fragment
``` bash
login2.ls6$ cd ~
login2.ls6$ tail -20 .bashrc > conda.init
login2.ls6$ nano conda.init [delete any lines not contained within the conda init lines]
login2.ls6$ more conda.init

## >>> conda initialize >>>
## !! Contents within this block are managed by 'conda init' !!
   __conda_setup="$('/work/07723/richherr/miniconda3/bin/conda' 'shell.bash' 'hook' 2> /dev/null
)"
   if [ $? -eq 0 ]; then
       eval "$__conda_setup"
   else
       if [ -f "/work/07723/richherr/miniconda3/etc/profile.d/conda.sh" ]; then
           . "/work/07723/richherr/miniconda3/etc/profile.d/conda.sh"
       else
           export PATH="/work/07723/richherr/miniconda3/bin:$PATH"
       fi
   fi
   unset __conda_setup
## <<< conda initialize <<<
```
:::

## Editing [.bashrc]{style="color: red;"} and Activating Conda {style="font-size: 0.80em"}

5.  Now we need to remove the conda init lines from our **.bashrc**
    file. Use **nano** editor again to edit the **.bashrc** file. Scroll
    to the bottom of the file and use **Ctl-k** to delete all of the
    conda init lines (see lines above). When finished use **Ctl-o** to
    save to your **.bashrc** file. Now you can use the **source**
    command to source the **conda.init** file rather than having conda
    active everytime you login to LS6. This gives you the ability to use
    **conda activate** in a bash shell.

::: fragment
``` bash
login2.ls6$ cd ~
login2.ls6$ nano .bashrc [delete all of the conda init lines - see above]
login2.ls6$ source conda.init
login2.ls6$ conda activate
(base) login1.ls6$ conda env list
# conda environments:
#
base                  *  /work/07723/richherr/miniconda3
>>>
```
:::

## Creating a New Conda Environment {style="font-size: 0.80em"}

6.  Let's create a new conda environment. But first, let's make sure our
    python path is the one determined by our conda base environment by
    using the **which** command. Then we will use the **create**
    argument to conda to create a new environment. Will create an
    environment for the AI software H2o.  We will use Python version 3.8 
    with H2o.
    
    [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/welcome.html) 

::: fragment
``` bash
(base) login1.ls6$ which python
/work/07723/richherr/miniconda3/bin/python
```
:::

::: fragment
``` bash
(base) login1.ls6$ conda create -n h2oai python=3.8

Collecting package metadata (current_repodata.json): done
Solving environment: done
## Package Plan ##
  environment location: /work/07723/richherr/miniconda3/envs/h2oai

(base) login1.ls6$ conda activate h2oai
(base) login1.ls6$ conda install -c h2oai h2o
```
:::

## Conda Installation of H2o

7.  The **conda install** will take a few minutes to install you will
    see

::: fragment
``` bash
The following packages will be downloaded:

    package                    |            build
    ---------------------------|-----------------
    alsa-lib-1.2.10            |       hd590300_0         542 KB  conda-forge
    cairo-1.16.0               |    h0c91306_1017         1.1 MB  conda-forge
    freetype-2.12.1            |       h267a509_2         620 KB  conda-forge
...
...
...
  xorg-xextproto     conda-forge/linux-64::xorg-xextproto-7.3.0-h0b41bf4_1003
  xorg-xproto        conda-forge/linux-64::xorg-xproto-7.0.31-h7f98852_1007
  xz                 conda-forge/linux-64::xz-5.2.6-h166bdaf_0
  zlib               conda-forge/linux-64::zlib-1.2.13-hd590300_5
  zstd               conda-forge/linux-64::zstd-1.5.5-hfc55251_0


Proceed ([y]/n)? y


Downloading and Extracting Packages

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
(h2oai) login1.ls6$
```
:::

## Mamba: A Fast, Reliable Alternative to Conda 

8. Mamba is an open-source package manager that is fully compatible with Conda. Like Conda, it is designed to manage and deploy software packages and their dependencies. Mamba is notably recognized for its speed in resolving dependencies and installing packages.

::: fragment
::: callout-note
Mamba was installed along with Conda during our earlier Miniforge installation
:::
:::

## Why Use Mamba Rather Than Conda?  
- Speed and Efficiency: Mamba uses a C++ library (libsolv) for dependency resolution, which is significantly faster than Conda's Python-based resolver. 
- Quick Installations: Mamba often installs packages more quickly than Conda, enhancing user efficiency, especially when dealing with environments that have numerous or complex dependencies.
- Compatibility with Conda: Mamba is compatible with Conda environments and repositories, allowing users to benefit from the extensive Conda package ecosystem without major modifications.

## Why Use Mamba Rather Than Conda?  
- Environment Management: Mamba can manage Conda environments, ensuring that users can switch between Mamba and Conda easily. 
- Lower Memory Usage: Mamba typically uses less memory than Conda, making it a lightweight alternative, especially beneficial for systems with limited resources.
- Optimized Computations: Mamba is designed to optimize computational resources, ensuring efficient use of CPU and memory during package management tasks.

## Using Mamba to install Within an Existing Conda Environment {style="font-size: 0.68em"}

8.  Let's use Mamba to install the Python module **dask** in our existing h2o conda environment. Dask is a parallel computing library in Python that enables performance optimizations through dynamic task scheduling and parallel execution, particularly suited for processing larger-than-memory datasets. 
    
    [https://docs.dask.org/en/stable/](https://docs.dask.org/en/stable/) 

::: fragment
``` bash
(h2oai) login1.ls6$ mamba install dask
Downloading and Extracting Packages

Preparing transaction: done
Verifying transaction: done
Executing transaction: done
```
:::

## Using Mamba to install Within an Existing Conda Environment {style="font-size: 0.68em"}

::: fragment
We can use **conda list** to list all the packages in the **h2oai** environment. 
:::

::: fragment
``` bash
(h2oai) c307-012.ls6$ conda list
# packages in environment at /work/07723/richherr/miniconda3/envs/h2oai:
#
# Name                    Version                   Build  Channel
_libgcc_mutex             0.1                 conda_forge    conda-forge
_openmp_mutex             4.5                       2_gnu    conda-forge
alsa-lib                  1.2.10               hd590300_0    conda-forge
aws-c-auth                0.7.4                hc8144f4_1    conda-forge
...
...
...
zict                      3.0.0              pyhd8ed1ab_0    conda-forge
zipp                      3.17.0             pyhd8ed1ab_0    conda-forge
zlib                      1.2.13               hd590300_5    conda-forge
zstd                      1.5.5                hfc55251_0    conda-forge
```
:::

## Using Our H2o Conda Environment with a SLURM Batch Job {style="font-size: 0.64em"}

9. Next we will use our H2o conda environment with the SLURM batch system on LS6.  We will build a build a randomForest classifier with H2o using the **Data Expo 2009 - Airline on-time performance** data. [https://community.amstat.org/jointscsg-section/dataexpo/dataexpo2009](https://community.amstat.org/jointscsg-section/dataexpo/dataexpo2009). Our target variable is whether or not a flight has been delayed.  We use a smaller subset of this data - 2GB compressed.  The full data is 12GB compressed. The smaller data has 43978 rows with 31 variables - we will use 8 of these variables as predictors.   Our Python script **h2o_randomForesty.py** is listed below.

::: fragment
``` bash
import h2o
from h2o.estimators import H2ORandomForestEstimator
# Initialize H2O cluster
h2o.init(ip="localhost", port=54321)
# Load the airlines dataset
data = h2o.import_file(path="http://h2o-public-test-data.s3.amazonaws.com/smalldata/airlines/allyears2k_headers.zip")
# Define features and target
features = ["Year", "Month", "DayOfWeek", "DepTime", "UniqueCarrier", "Origin", "Dest", "Distance"]
target = "IsArrDelayed"
# Train a model
model = H2ORandomForestEstimator()
model.train(x=features, y=target, training_frame=data)
# Train a model
model = H2ORandomForestEstimator(ntrees=100, max_depth=50)
model.train(x=features, y=target, training_frame=data)
# Display model performance
print(model)
# Shutdown H2O cluster
h2o.cluster().shutdown()
```
:::

## Creating the SLURM Batch Job {style="font-size: 0.64em"}

10. We need to create a SLURM batch script **h2o_randomForest.sh** that activates our **h2oai** environment and then submits the Python script to a compute note. 

::: fragment
``` bash
#!/bin/bash
#SBATCH --job-name=h2o_Job
#SBATCH --output=h2o_Job_%A_%a.out
#SBATCH --error=h2o_Job_%A_%a.err
#SBATCH -N 1
#SBATCH -p development
#SBATCH -n 1
#SBATCH -t 02:00:00
#SBATCH --mail-user=richherr@unt.edu
#SBATCH --mail-type=begin
#SBATCH --mail-type=end

# Load module or activate conda environment
source ~/conda.init
conda activate h2oai

# Initialize H2O cluster
srun python h2o_randomForest.py
```
:::

## Submitting the SLURM Batch Job {style="font-size: 0.64em"}

11. Make sure the python script and the slurm script are in the same directory before submitting the batch script 

::: fragment
``` bash
login1.ls6$ pwd
/home1/07723/richherr/work/h2o
login1.ls6$ sbatch h2o_randomForest.sh

-----------------------------------------------------------------
           Welcome to the Lonestar6 Supercomputer
-----------------------------------------------------------------

No reservation for this job
--> Verifying valid submit host (login1)...OK
--> Verifying valid jobname...OK
--> Verifying valid ssh keys...OK
--> Verifying access to desired queue (development)...OK
--> Checking available allocation (TRA21001)...OK
--> Verifying that quota for filesystem /home1/07723/richherr is at  8.61% allocated...OK
--> Verifying that quota for filesystem /work/07723/richherr/ls6 is at 12.70% allocated...OK
Submitted batch job 1218654
```
:::

::: fragment
``` bash
login1.ls6$ squeue -u richherr      

login1.ls6.tacc.utexas.edu: 
Mon Oct  9 
01:36:02 2023

  JOBID    PARTITION          NAME     USER  ST      TIME  NODES           NODELIST(REASON)
1218502       normal  h2o_arrayJob richherr   R   0:01:04      1           c302-017
```
:::

## Logging Into the Compute Node and Checking the Resource Utilization {style="font-size: 0.64em"}

12. We want to make sure that our H2o randomForest model is maximally using the resources on the compute node.   To do this we **ssh** login to the node and use **htop** to view the processor activity.  Here we see that all of the processors are being utilized. 

::: fragment
``` bash
  login1.ls6$ ssh c302-017
c302-017.ls6$ htop
```
:::

::: fragment
![htop view](/work/richherr/UNT_workshops/h2o_htop_view.png){.fragment width="860" height="425"}
:::

## Our H2o Output {style="font-size: 0.64em"}

13. The Area Under the Curve (AUC) is about 0.775 and the Accuracy is about 0.706.  Ideally, I would have split the data into a **training** data set and a **hold-out** data set.  For the sake of brevity, I have trained on the unsplit data.    

::: fragment
``` bash
Checking whether there is an H2O instance running at http://localhost:54321..... not found.
Attempting to start a local H2O server...
  Java Version: openjdk version "1.8.0_345"; OpenJDK Runtime Environment (build 1.8.0_345-b01); OpenJDK 64-Bit Server VM (build 25.345-b01, mixed mode)
  Starting server from /work/07723/richherr/miniconda3/envs/h2oai/lib/python3.8/site-packages/h2o/backend/bin/h2o.jar
  Ice root: /tmp/tmpzwab4jj7
  JVM stdout: /tmp/tmpzwab4jj7/h2o_richherr_started_from_python.out
  JVM stderr: /tmp/tmpzwab4jj7/h2o_richherr_started_from_python.err
  Server is running at http://127.0.0.1:54321
Connecting to H2O server at http://127.0.0.1:54321 ... successful.
--------------------------  -------------------------------
H2O_cluster_uptime:         01 secs
H2O_cluster_timezone:       America/Chicago
H2O_data_parsing_timezone:  UTC
H2O_cluster_version:        3.42.0.4
H2O_cluster_version_age:    5 days
H2O_cluster_name:           H2O_from_python_richherr_u1qlyr
H2O_cluster_total_nodes:    1
H2O_cluster_free_memory:    26.63 Gb
H2O_cluster_total_cores:    128
H2O_cluster_allowed_cores:  128
H2O_cluster_status:         locked, healthy
H2O_connection_url:         http://127.0.0.1:54321
H2O_connection_proxy:       {"http": null, "https": null}
H2O_internal_security:      False
Python_version:             3.8.17 final
--------------------------  -------------------------------
Parse progress: |████████████████████████████████████████████████████████████████| (done) 100%
drf Model Build progress: |██████████████████████████████████████████████████████| (done) 100%
Model Details
=============
H2ORandomForestEstimator : Distributed Random Forest
Model Key: DRF_model_python_1696829720302_1


Model Summary: 
    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves
--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------
    100                100                         1.01086e+07            28           41           33.69         5421          7078          6275.46

ModelMetricsBinomial: drf
** Reported on train data. **

MSE: 0.19245778290965485
RMSE: 0.4387001058920032
LogLoss: 0.5685405955092597
Mean Per-Class Error: 0.3303219396045163
AUC: 0.7752525578240561
AUCPR: 0.8142937589826034
Gini: 0.5505051156481122

Confusion Matrix (Act/Pred) for max f1 @ threshold = 0.3670802474595691
       NO     YES    Error    Rate
-----  -----  -----  -------  -----------------
NO     9351   10186  0.5214   (10186.0/19537.0)
YES    3404   21037  0.1393   (3404.0/24441.0)
Total  12755  31223  0.309    (13590.0/43978.0)

Maximum Metrics: Maximum metrics at their respective thresholds
metric                       threshold    value     idx
---------------------------  -----------  --------  -----
max f1                       0.36708      0.755857  269
max f2                       0.0951444    0.86438   374
max f0point5                 0.603504     0.745612  170
max accuracy                 0.501732     0.705876  212
max precision                0.999861     0.994286  0
max recall                   0.00661542   1         399
max specificity              0.999861     0.999898  0
max absolute_mcc             0.577354     0.410408  180
max min_per_class_accuracy   0.532229     0.704305  199
max mean_per_class_accuracy  0.577354     0.706149  180
max tns                      0.999861     19535     0
max fns                      0.999861     24093     0
max fps                      0.00661542   19537     399
max tps                      0.00661542   24441     399
max tnr                      0.999861     0.999898  0
max fnr                      0.999861     0.985762  0
max fpr                      0.00661542   1         399
max tpr                      0.00661542   1         399

Gains/Lift Table: Avg response rate: 55.58 %, avg score: 54.78 %
group    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score     cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov
-------  --------------------------  -----------------  --------  -----------------  ---------------  --------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------
1        0.010005                    0.994368           1.77073   1.77073            0.984091         0.999166  0.984091                    0.999166            0.0177161       0.0177161                  77.0727   77.0727            0.0173578
2        0.02001                     0.980109           1.77891   1.77482            0.988636         0.986681  0.986364                    0.992923            0.017798        0.0355141                  77.8906   77.4817            0.0348999
3        0.030015                    0.971524           1.72983   1.75982            0.961364         0.975546  0.97803                     0.987131            0.017307        0.0528211                  72.9833   75.9822            0.0513367
4        0.04002                     0.963636           1.70121   1.74517            0.945455         0.96758   0.969886                    0.982243            0.0170206       0.0698417                  70.1207   74.5168            0.0671289
5        0.0500023                   0.956577           1.69279   1.73471            0.940774         0.960065  0.964075                    0.977816            0.0168978       0.0867395                  69.2786   73.4711            0.0826959
6        0.100005                    0.914274           1.64061   1.68766            0.911778         0.936153  0.937926                    0.956984            0.0820343       0.168774                   64.0611   68.7661            0.1548
7        0.150007                    0.866243           1.53424   1.63652            0.85266          0.890093  0.909504                    0.934687            0.0767154       0.245489                   53.4237   63.652             0.214932
8        0.200009                    0.820164           1.45241   1.59049            0.807185         0.842914  0.883925                    0.911744            0.0726239       0.318113                   45.2411   59.0493            0.265853
9        0.300014                    0.728802           1.35667   1.51255            0.753979         0.774261  0.840609                    0.865916            0.135674        0.453787                   35.6675   51.2553            0.346145
10       0.399995                    0.638516           1.21703   1.43868            0.67637          0.68357   0.799557                    0.820337            0.121681        0.575467                   21.7029   43.8685            0.394989
11       0.5                         0.55136            1.06619   1.36418            0.592542         0.594921  0.758152                    0.775252            0.106624        0.682092                   6.61927   36.4183            0.40989
12       0.600005                    0.464988           0.914405  1.28922            0.508186         0.507895  0.716489                    0.730691            0.0914447       0.773536                   -8.55945  28.9217            0.390622
13       0.699986                    0.376043           0.797985  1.21905            0.443484         0.420355  0.677495                    0.686364            0.079784        0.85332                    -20.2015  21.9053            0.345156
14       0.799991                    0.281715           0.624741  1.14476            0.347203         0.329347  0.636206                    0.641735            0.062477        0.915797                   -37.5259  14.476             0.260681
15       0.899995                    0.181927           0.503639  1.07352            0.2799           0.23302   0.596614                    0.59632             0.0503662       0.966163                   -49.6361  7.35203            0.148945
16       1                           0                  0.33835   1                  0.18804          0.111008  0.555755                    0.547786            0.0338366       1                          -66.165   0                  0

Scoring History: 
    timestamp            duration    number_of_trees    training_rmse    training_logloss    training_auc    training_pr_auc    training_lift    training_classification_error
--  -------------------  ----------  -----------------  ---------------  ------------------  --------------  -----------------  ---------------  -------------------------------
    2023-10-09 00:35:27  0.476 sec   0                  nan              nan                 nan             nan                nan              nan
    2023-10-09 00:35:29  2.382 sec   1                  0.515299         5.63414             0.692323        0.704275           1.34254          0.353912
    2023-10-09 00:35:30  3.005 sec   2                  0.510108         4.89216             0.693204        0.709189           1.36054          0.374477
    2023-10-09 00:35:30  3.462 sec   3                  0.500814         4.3428              0.704674        0.72269            1.39428          0.360016
    2023-10-09 00:35:31  4.039 sec   4                  0.49432          3.66988             0.708099        0.727025           1.40591          0.360578
    2023-10-09 00:35:31  4.523 sec   5                  0.487988         3.18919             0.71399         0.73376            1.42353          0.359061
    2023-10-09 00:35:36  9.138 sec   28                 0.445118         0.635559            0.765479        0.802987           1.68878          0.316067
    2023-10-09 00:35:40  13.143 sec  51                 0.440962         0.581706            0.77179         0.810237           1.77073          0.31584
    2023-10-09 00:35:44  17.161 sec  91                 0.438949         0.569314            0.774856        0.813745           1.77482          0.307267
    2023-10-09 00:35:45  18.065 sec  100                0.4387           0.568541            0.775253        0.814294           1.77073          0.309018

Variable Importances: 
variable       relative_importance    scaled_importance    percentage
-------------  ---------------------  -------------------  ------------
DepTime        150226                 1                    0.370335
Origin         66943.8                0.44562              0.165029
Dest           59909.6                0.398797             0.147688
DayOfWeek      47535.5                0.316427             0.117184
Year           37655.7                0.25066              0.0928282
Distance       25339.4                0.168675             0.0624662
UniqueCarrier  12058.6                0.0802699            0.0297267
Month          5980.7                 0.0398114            0.0147435
H2O session _sid_bc8c closed.
```
:::

# Further Package Management Details to Consider

## Conda Channels

14. Conda channels are repositories that host packages and make them available for Conda users. 
- When you install a package using Conda, the package is retrieved from one of these channels. 
- Channels serve as the distribution layer in the Conda ecosystem, providing a scalable and manageable way to share packages. 

## Example Using a Conda Channel with [-c]{style="color: red;"} Argument {style="font-size: 0.64em"}
- Suppose you want to install a package named **biopython** from the **bioconda** channel. bioconda is a channel that provides bioinformatics software for Conda. To install biopython from bioconda, you would use the following (you can use either conda or mamba - I use mamba since it is much faster)

::: fragment
``` bash
login1.ls6$ mamba install -c bioconda biopython

```
:::
- You can also specify multiple channels by using the -c flag multiple times. Conda will search the channels in the order they are listed. For example, if you want Conda to search the bioconda channel first and then the conda-forge channel, you would use

::: fragment
``` bash
login1.ls6$ conda install -c bioconda -c conda-forge package_name
```
:::
- When you run the command **conda config --add channels channel_name**, Conda will add that channel to the top of your channel list, meaning it will be the highest-priority channel. For example, when adding **conda-forge**, conda will search in the conda-forge channel first, and if the package is not found there, it will search in the next channel in the list, and so on.

::: fragment
``` bash
login1.ls6$ conda config --add channels conda-forge
```
:::

## Creating and Sharing an Environment{style="font-size: 0.64em"}

15. Channels can host environment files, which are typically YAML files that specify a list of packages to be installed together. These environment files can be shared and used by others to recreate the same environment, ensuring consistency and reproducibility across different systems and use cases.
- Imagine a research team that uses several packages like NumPy, Pandas, and Matplotlib together. A team member can create an environment file, say **data_science_env.yml**, that specifies these packages:

::: fragment
``` bash
name: data_science_env
channels:
  - conda-forge
  - defaults
dependencies:
  - numpy
  - pandas
  - matplotlib
```
:::

- This file can be local or retreived from a remote location

::: fragment
``` bash
login1.ls6$ conda env create -f data_science_env.yml
login1.ls6$ conda env create -f https://raw.githubusercontent.com/user/repo/branch/data_science_env.yml
```
:::

## Creating and Using [.yml]{style="color: red;"} Files {style="font-size: 0.64em"}

16. Creating a **.yml** file (often referred to as an environment file) from an existing Conda environment is a common practice to ensure reproducibility and consistency across different systems and use cases. This file can then be used to recreate the same environment on a different system or by a different user.
- Here’s how you can create an .yml file from an existing Conda environment and use it to create a new environment:

::: fragment
``` bash
(h2oai) login2.ls6$ conda activate h2oai
(h2oai) login2.ls6$ conda env export --name h2oai > h2oai.yml
(h2oai) login2.ls6$ more h2oai.yml

name: h2oai
channels:
  - h2oai
  - conda-forge
  - bioconda
  - defaults
dependencies:
  - _libgcc_mutex=0.1=conda_forge
  - _openmp_mutex=4.5=2_gnu
...
...
  - zipp=3.17.0=pyhd8ed1ab_0
  - zstd=1.5.5=hfc55251_0
prefix: /work/07723/richherr/miniconda3/envs/h2oai
```
:::
